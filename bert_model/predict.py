import torch
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import BertConfig

# ====== 1. è¨­å®šè¨­å‚™èˆ‡è¼‰å…¥æ¨¡å‹ã€Tokenizer ======
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_path = "C:/Users/user/PycharmProjects/PythonProject1/bert_model/Saved_model"


# æ˜ç¢ºæŒ‡å®šä¸­æ–‡ BERT çš„è¨­å®š
config = BertConfig.from_pretrained("bert-base-chinese", num_labels=6)
model = BertForSequenceClassification(config)
model.load_state_dict(torch.load(f"{model_path}/pytorch_model.bin", map_location=device))
model = model.to(device)

# è¼‰å…¥ tokenizerï¼ˆé€™è£¡å»ºè­°ä¹Ÿå¾åŸå§‹ bert-base-chineseï¼‰
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

# å¤šæ¨™ç±¤åˆ†é¡çš„æ¨™ç±¤åç¨±
label_columns = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]

# ====== 2. é æ¸¬å‡½æ•¸ ======
def predict_user_input(input_text, model, tokenizer, device, threshold=0.5):
    model.eval()

    # Tokenize å–®ç­†æ–‡å­—
    encoding = tokenizer(
        [input_text],
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probs = torch.sigmoid(logits)  # æ©Ÿç‡å€¼

    # åˆ¤æ–·æ˜¯å¦è¶…éé–¾å€¼ï¼ˆå¤šæ¨™ç±¤ï¼‰
    predictions = (probs > threshold).int().squeeze().tolist()

    # å›å‚³ dict æ ¼å¼çµæœ
    result = {label: bool(pred) for label, pred in zip(label_columns, predictions)}
    return result

# ====== 3. æ¸¬è©¦ç”¨ä¾‹ ======
if __name__ == "__main__":
    input_text = input("è«‹è¼¸å…¥ä¸€æ®µä¸­æ–‡è©•è«–é€²è¡Œé æ¸¬ï¼š\n> ")
    prediction = predict_user_input(input_text, model, tokenizer, device)

    print("\nğŸ“Š é æ¸¬çµæœï¼š")
    for label, present in prediction.items():
        print(f"- {label}: {'âœ… æ˜¯' if present else 'âŒ å¦'}")
